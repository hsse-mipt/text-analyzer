{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Таблица сопряжённости** (матрица неточности) содержит сводные показатели качества работы классификатора.\n",
    "* **Строки** соответствуют **фактическим** классам тестового набора;\n",
    "* **Cтолбцы** соответствуют **предсказанным** классом.\n",
    "\n",
    "Таблица содержит четыре сводных показателя, каждый из которых отражает количество объектов в одной и четырех\n",
    "категорий:\n",
    "* **Истинно позитивный** (*True positive*, **TP**) -- объект\n",
    "класса `1` был верно помечен меткой `1`;\n",
    "* **Ложно позитивный** (*False positive*, **FP**) -- объект\n",
    "фактически принадлежит классу `0`, но помечен меткой `1`;\n",
    "* **Истинно отрицательный** (*True negative*, **TN**) -- классификатор\n",
    "верно определил, что объект класса `0` принадлежит классу `0`;\n",
    "* **Ложно отрицательный** (*False negative*, **FN**) -- классификатор\n",
    "пометил объект меткой `0`, однако на самом деле объект принадлежит классу `1`.\n",
    "\n",
    "\n",
    "|                    | Предсказано `0` | Предсказано `1` |\n",
    "|:-------------------|:----------------|:----------------|\n",
    "| **Фактически** `0` | TN              | FP              |\n",
    "| **Фактически** `1` | FN              | TP              |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    :param y_true: реальные классы\n",
    "    :param y_pred: предсказанные классы\n",
    "    \"\"\"\n",
    "    confusion_matrix = np.array([[0,0], [0,0]]) # столбец - предсказанный класс; строка - реальный\n",
    "    for value, prediction in zip(y_true, y_pred):\n",
    "        confusion_matrix[value][prediction] += 1\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    TN, FP, FN, TP = get_confusion_matrix(y_true, y_pred).ravel()\n",
    "    return TP / (TP + (FP + FN) / 2)\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    TN, FP, FN, TP = get_confusion_matrix(y_true, y_pred).ravel()\n",
    "    return (TN + TP) / (TN + FP + FN + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math\n",
    "\n",
    "def test_computations(y_true, y_pred):\n",
    "    assert math.isclose(f1_score(y_true, y_pred), sklearn.metrics.f1_score(y_true, y_pred))\n",
    "    assert math.isclose(accuracy_score(y_true, y_pred), sklearn.metrics.accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_true = [0,1,1,0,0,1,1,0,0,0]\n",
    "y_pred = [0,1,1,0,0,1,0,1,0,1]\n",
    "test_computations(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
