{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Таблица сопряжённости** (матрица неточности, или Confusion matrix) содержит сводные показатели качества работы классификатора.\n",
    "\n",
    "* **Строки** соответствуют **фактическим** классам тестового набора;\n",
    "* **Cтолбцы** соответствуют **предсказанным** классом.\n",
    "\n",
    "Таблица содержит четыре сводных показателя, каждый из которых отражает количество объектов в одной и четырех\n",
    "категорий:\n",
    "* **Истинно позитивный** (*True positive*, **TP**) -- объект\n",
    "класса `1` был верно помечен меткой `1`;\n",
    "* **Ложно позитивный** (*False positive*, **FP**) -- объект\n",
    "фактически принадлежит классу `0`, но помечен меткой `1`;\n",
    "* **Истинно отрицательный** (*True negative*, **TN**) -- классификатор\n",
    "верно определил, что объект класса `0` принадлежит классу `0`;\n",
    "* **Ложно отрицательный** (*False negative*, **FN**) -- классификатор\n",
    "пометил объект меткой `0`, однако на самом деле объект принадлежит классу `1`.\n",
    "\n",
    "|                    | **Предсказано** `0` | **Предсказано** `1` |\n",
    "|:-------------------|:--------------------|:--------------------|\n",
    "| **Фактически** `0` | TN                  | FP                  |\n",
    "| **Фактически** `1` | FN                  | TP                  |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    :param y_true: реальные классы\n",
    "    :param y_pred: предсказанные классы\n",
    "    :return: f1-мера\n",
    "    \"\"\"\n",
    "    confusion_matrix = np.array([[0,0], [0,0]])\n",
    "    for value, prediction in zip(y_true, y_pred):\n",
    "        confusion_matrix[value][prediction] += 1\n",
    "\n",
    "    TP = confusion_matrix[1][1]\n",
    "    FP, FN = confusion_matrix[0][1], confusion_matrix[1][0]\n",
    "\n",
    "    return TP / (TP + (FP + FN) / 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def test_computations():\n",
    "    y_true, y_pred = [0,1,1,0,0,1,1,0,0,0], [0,1,1,0,0,1,0,1,0,1]\n",
    "    assert round(metrics.f1_score(y_true, y_pred), ndigits=12) == round(f1_score(y_true, y_pred), ndigits=12)\n",
    "\n",
    "test_computations()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
